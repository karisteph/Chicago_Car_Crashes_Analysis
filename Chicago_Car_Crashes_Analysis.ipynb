{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Car Crashes Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction text to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three distinct datasets are used in this analysis:\n",
    "- Crashes\n",
    "- People\n",
    "- Socially Disadvantaged Districts\n",
    "\n",
    "These datasets are large.  Specifying the specific columns to import for each dataset will limit the time it takes to bring them into dataframes, and will minimize system resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_usecols = ['LATITUDE','LONGITUDE','CRASH_RECORD_ID','CRASH_DATE','DEVICE_CONDITION',\\\n",
    "                   'WEATHER_CONDITION','LIGHTING_CONDITION','ALIGNMENT','ROADWAY_SURFACE_COND',\\\n",
    "                   'INJURIES_FATAL','INJURIES_INCAPACITATING','CRASH_HOUR','CRASH_DAY_OF_WEEK']\n",
    "\n",
    "people_usecols = ['CRASH_RECORD_ID','SAFETY_EQUIPMENT','PHYSICAL_CONDITION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV files into DataFrames\n",
    "crashes_df = pd.read_csv('Data/Traffic_Crashes_-_Crashes_20240412.csv', usecols=crashes_usecols)\n",
    "people_df = pd.read_csv('Data/Traffic_Crashes_-_People_20240412.csv', usecols=people_usecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SHP file into a GeoDataFrame\n",
    "districts_gdf = gpd.read_file('Data\\SD_geo.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing discussion - for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crashes dataset\n",
    "- Create flag for records within disadvantaged districts\n",
    "- Create bins for: (1) roadway conditions, (2) roadway alignment, (3) roadway devices, (4) weather, (5) lighting, and (6) surface conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic analysis\n",
    "- Drop records without locations\n",
    "- Convert the Crashes DataFrame into a GeoDataFrame so that the coordinates can be turned into Points using the Shapely package\n",
    "- Ensure that the resulting GDF uses the same reference system as the imported districts GDF\n",
    "- Spatially join the two GDFs, checking to see if each point is within one of the districts and assigning 1's to those\n",
    "- Drop unnecessary columns and convert back to a DataFrame, which is computationally more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop values with no location information\n",
    "crashes_df.dropna(subset=['LATITUDE','LONGITUDE'], inplace=True)\n",
    "\n",
    "# Convert DataFrame into GeoDataFrame\n",
    "crashes_df['geometry'] = crashes_df.apply(lambda row: Point(row['LONGITUDE'], row['LATITUDE']), axis=1)\n",
    "crashes_gdf = gpd.GeoDataFrame(crashes_df, geometry='geometry')\n",
    "\n",
    "# Ensure that both GeoDataFrames use the same CRS\n",
    "crashes_gdf.crs = districts_gdf.crs\n",
    "\n",
    "# Spatial join the GeoDataFrames\n",
    "joined_gdf = gpd.sjoin(crashes_gdf, districts_gdf, how='left', predicate='within')\n",
    "\n",
    "# Add flag for crashes that are within a district\n",
    "joined_gdf['WITHIN_DISTRICT'] = joined_gdf['index_right'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "\n",
    "# Drop the geometry, index_right, LATITUDE and LONGITUDE columns\n",
    "joined_gdf.drop(columns=['LATITUDE','LONGITUDE','geometry','index_right'], axis=1, inplace=True)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "crashes_flag_df = pd.DataFrame(joined_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare datetime data for analysis\n",
    "- Create a helper function to convert each date to a season\n",
    "- Convert each date string to a season\n",
    "- Group days of the week into weekday/weekend categories\n",
    "- Group hours of the day into thematic blocks\n",
    "- Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert dates to seasons\n",
    "\n",
    "def get_season(date):\n",
    "    year = date.year\n",
    "    seasons = [('winter', datetime(year, 1, 1).date(), datetime(year, 2, 28).date()),\n",
    "               ('spring', datetime(year, 3, 1).date(), datetime(year, 5, 31).date()),\n",
    "               ('summer', datetime(year, 6, 1).date(), datetime(year, 8, 31).date()),\n",
    "               ('autumn', datetime(year, 9, 1).date(), datetime(year, 11, 30).date()),\n",
    "               ('winter', datetime(year, 12, 1).date(), datetime(year, 12, 31).date())]\n",
    "    if date.year % 4 == 0:  # leap year check\n",
    "        seasons[0] = ('winter', datetime(year, 1, 1).date(), datetime(year, 2, 29).date())\n",
    "    \n",
    "    for season, start, end in seasons:\n",
    "        if start <= date <= end:\n",
    "            return season\n",
    "\n",
    "    return 'Date is out of range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date field to season\n",
    "crashes_df['SEASON'] = crashes_df['CRASH_DATE'].apply(lambda x: \n",
    "                                                      get_season(datetime.strptime(x[:10],'%m/%d/%Y').date()))\n",
    "\n",
    "# Group CRASH_DAY_OF_WEEK into weekdays and weekends\n",
    "weekday_mask = [2,3,4,5,6]\n",
    "crashes_flag_df['CRASH_WEEKEND'] = crashes_flag_df['CRASH_DAY_OF_WEEK'].apply(lambda x: 0 if x in weekday_mask else 1)\n",
    "\n",
    "# Group hours into time blocks (extra bin at end to ensure correct bin treatment)\n",
    "bins = [-1, 6, 9, 15, 19, 23, 24]\n",
    "labels = ['NIGHT', 'MORNING_RUSH', 'MIDDAY', 'EVENING_RUSH', 'NIGHT', 'NIGHT']\n",
    "crashes_flag_df['TIME_BLOCK'] = pd.cut(crashes_flag_df['CRASH_HOUR'], bins=bins, labels=labels, right=True, ordered=False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "crashes_df.drop(columns=['CRASH_DATE','CRASH_DAY_OF_WEEK','CRASH_HOUR'], axis=1, inplace=True)                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category flags\n",
    "- Convert string categories into 1/0 where 1's signify potential dangerous conditions\n",
    "- Categories include malfunctioning road device, bad weather conditions, poor lighting, non-straight and level roads and unsafe surfaces\n",
    "- Create a target column equal to 1 if there are any fatal or incapacitating injuries\n",
    "- Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_mask = ['NO CONTROLS','FUNCTIONING PROPERLY']\n",
    "weather_mask = ['CLEAR','UNKNOWN']\n",
    "lighting_mask = ['DAYLIGHT']\n",
    "alignment_mask = ['STRAIGHT AND LEVEL']\n",
    "surface_mask = ['DRY','UNKNOWN']\n",
    "\n",
    "crashes_flag_df['DEVICE_FLAG'] = crashes_flag_df['DEVICE_CONDITION'].apply(lambda x: 0 if x in device_mask else 1)\n",
    "crashes_flag_df['WEATHER_FLAG'] = crashes_flag_df['WEATHER_CONDITION'].apply(lambda x: 0 if x in weather_mask else 1)\n",
    "crashes_flag_df['LIGHTING_FLAG'] = crashes_flag_df['LIGHTING_CONDITION'].apply(lambda x: 0 if x in lighting_mask else 1)\n",
    "crashes_flag_df['ALIGNMENT_FLAG'] = crashes_flag_df['ALIGNMENT'].apply(lambda x: 0 if x in alignment_mask else 1)\n",
    "crashes_flag_df['SURFACE_FLAG'] = crashes_flag_df['ROADWAY_SURFACE_COND'].apply(lambda x: 0 if x in surface_mask else 1)\n",
    "\n",
    "# Flag for serious accidents (fatal + incapacitating), which is the target\n",
    "crashes_flag_df['TARGET'] = crashes_flag_df.apply(lambda row: 1 if \n",
    "                                                  (row['INJURIES_FATAL']+row['INJURIES_INCAPACITATING'] > 0) else 0,\n",
    "                                                  axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "crashes_flag_df = crashes_flag_df.drop(columns=['DEVICE_CONDITION', 'WEATHER_CONDITION', 'LIGHTING_CONDITION',\\\n",
    "                                                'ALIGNMENT', 'ROADWAY_SURFACE_COND',\\\n",
    "                                               'INJURIES_FATAL', 'INJURIES_INCAPACITATING'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### People Dataset\n",
    "This dataset includes all people involved with any crash (e.g. the driver of each car, each passenger), so there is more than one line per crash.  Since this analysis predicts the effect of crashes, all relevant information must be extracted and processed into per-crash form.  This is accomplished by converting each feature into a single flag per crash.\n",
    "- Flag vehicle operators with compromised physical features (e.g. alcohol, drugs, tired)\n",
    "- Flag participants that failed to use vehicle safety equipment properly\n",
    "- Group people by CRASH_ID and apply flag if applicable\n",
    "- Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masks to assist in binning the PHYSICAL_CONDITION and SAFETY_EQUIPMENT fields\n",
    "PhysicalMask = ['NORMAL', 'UNKNOWN']\n",
    "SafetyMask = ['SAFETY BELT USED', 'USAGE UNKNOWN', 'CHILD RESTRAINT USED', 'CHILD RESTRAINT - FORWARD FACING'\\\n",
    "             'BICYCLE HELMET (PEDACYCLIST INVOLVED ONLY)', 'CHILD RESTRAINT - TYPE UNKNOWN',\\\n",
    "             'CHILD RESTRAINT - REAR FACING', 'HELMET USED', 'DOT COMPLIANT MOTORCYCLE HELMET',\\\n",
    "             'BOOSTER SEAT', 'WHEELCHAIR', 'STRETCHER']\n",
    "\n",
    "# Bin all problematic physical and safety conditions and tag with a 1 \n",
    "people_df['PHYSICAL_FLAG'] = people_df['PHYSICAL_CONDITION'].apply(lambda x: 0 if x in PhysicalMask else 1)\n",
    "people_df['SAFETY_FLAG'] = people_df['SAFETY_EQUIPMENT'].apply(lambda x: 0 if x in SafetyMask else 1)\n",
    "\n",
    "# Drop the original columns\n",
    "people_df = people_df.drop(columns=['PHYSICAL_CONDITION', 'SAFETY_EQUIPMENT'], axis=1)\n",
    "\n",
    "# For each crash, tag if at least one element had a safety or physical problem\n",
    "safety_flag = people_df.groupby('CRASH_RECORD_ID')['SAFETY_FLAG'].max().reset_index()\n",
    "safety_flag.rename(columns={'SAFETY_FLAG': 'SAFETY_PROBLEM'}, inplace=True)\n",
    "people_df = people_df.drop('SAFETY_FLAG', axis=1).merge(safety_flag, on='CRASH_RECORD_ID', how='left')\n",
    "\n",
    "physical_flag = people_df.groupby('CRASH_RECORD_ID')['PHYSICAL_FLAG'].max().reset_index()\n",
    "physical_flag.rename(columns={'PHYSICAL_FLAG': 'PHYSICAL_PROBLEM'}, inplace=True)\n",
    "people_df = people_df.drop('PHYSICAL_FLAG', axis=1).merge(physical_flag, on='CRASH_RECORD_ID', how='left')\n",
    "\n",
    "# Drop remaining duplicate rows\n",
    "people_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "combined_df = crashes_flag_df.merge(people_df, on='CRASH_RECORD_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14723"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.TARGET.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the future:\n",
    "- Proximity to holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
